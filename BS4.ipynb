{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Request 基本用法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1 无参数的request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import requests\n",
    "\n",
    "# 下载新浪新闻首页的内容\n",
    "url = 'http://news.sina.com.cn/china/'\n",
    "# 用get函数发送GET请求，获取响应\n",
    "res = requests.get(url)\n",
    "# 设置响应的编码格式utf-8（默认格式为ISO-8859-1），防止中文出现乱码\n",
    "res.encoding = 'utf-8'\n",
    "\n",
    "print(type(res))\n",
    "print(res)\n",
    "print(res.text)\n",
    "\n",
    "# 输出：\n",
    "'''\n",
    "<class 'requests.models.Response'>\n",
    "<Response [200]>\n",
    "<!DOCTYPE html>\n",
    "<!-- [ published at 2017-04-19 23:30:28 ] -->\n",
    "<html>\n",
    "<head>\n",
    "<meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
    "<title>国内新闻_新闻中心_新浪网</title>\n",
    "<meta name=\"keywords\" content=\"国内时政,内地新闻\">\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1 带参数的request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "print(\"a\")\n",
    "#dic {key:value}\n",
    "para = {'id': '991161'} \n",
    "r=requests.get('http://music.163.com/#/artist',params=para)\n",
    "print(r.text)\n",
    "print(type(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Beautifulsoup4库\n",
    "\n",
    "##1 Beautifulsoup4 库的安装\n",
    ">pip install beautifulsoup4\n",
    "\n",
    "然后我们安装lxml，这是一个解析器，BeautifulSoup可以使用它来解析HTML，然后提取内容。\n",
    ">pip install lxml\n",
    "\n",
    "##2 Beautifulsoup4的使用\n",
    "\n",
    "网上找到的几个官方文档：BeautifulSoup4.4.0中文官方文档，BeautifulSoup4.2.0中文官方文档。不同版本的用法差不多，几个常用的语法都一样。\n",
    "\n",
    "首先来看BeautifulSoup的对象种类，在使用的过程中就会了解你获取到的东西接下来应该如何操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.1 自定义测试 html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <h1 id=\"title\">Hello World</h1>\n",
    "        <a href=\"#link1\" class=\"link\">This is link1</a>\n",
    "        <a href=\"#link2\" class=\"link\">This is link2</a>\n",
    "    </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.2 从html文本中获取soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# 这里指定解析器为html.parser（python默认的解析器），指定html文档编码为utf-8\n",
    "soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "print(type(soup))\n",
    "\n",
    "# 输出：<class 'bs4.BeautifulSoup'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.3 soup.select()函数用法\n",
    "####(1)获取指定标签的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = soup.select('h1')\n",
    "print(type(header))\n",
    "print(header)\n",
    "print(header[0])\n",
    "print(type(header[0]))\n",
    "print(header[0].text)\n",
    "\n",
    "# 输出：\n",
    "'''\n",
    "<type 'list'>\n",
    "[<h1 id=\"title\">Hello World</h1>]\n",
    "<h1 id=\"title\">Hello World</h1>\n",
    "<class 'bs4.element.Tag'>\n",
    "Hello World\n",
    "'''\n",
    "\n",
    "alinks = soup.select('a')\n",
    "print([x.text for x in alinks])\n",
    "\n",
    "# 输出：[u'This is link1', u'This is link2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####(2) 获取指定id的标签的内容（用'#'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.select('#title')\n",
    "print(type(title))\n",
    "print(title[0].text)\n",
    "\n",
    "# 输出：\n",
    "'''\n",
    "<type 'list'>\n",
    "Hello World\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####(3) 获取指定class的标签的内容（用'.'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alinks = soup.select('.link')\n",
    "print([x.text for x in alinks])\n",
    "\n",
    "# 输出：[u'This is link1', u'This is link2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####(4) 获取a标签的链接（href属性值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alinks[0]['href'])\n",
    "\n",
    "# 输出：#link1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####(5) 获取一个标签下的所有子标签的text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = soup.select('body')[0]\n",
    "print(body.text)\n",
    "\n",
    "# 输出：\n",
    "'''\n",
    "Hello World\n",
    "This is link1\n",
    "This is link2\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####(6) 获取不存在的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = soup.select('aa')\n",
    "print(aa)\n",
    "\n",
    "# 输出：[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####(7) 获取自定义属性值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html2 = '<a href=\"www.test.com\" qoo=\"123\" abc=\"456\">This is a link.</a>'\n",
    "soup2 = BeautifulSoup(html2,'html.parser')\n",
    "alink = soup2.select('a')[0]\n",
    "print(alink['qoo'])\n",
    "print(alink['abc'])\n",
    "\n",
    "# 输出：\n",
    "'''\n",
    "123\n",
    "456\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.3 soup.find()和soup.find_all()函数用法\n",
    "####(1) find()和find_all()函数原型：\n",
    "find和find_all函数都可根据多个条件从html文本中查找标签对象，只不过find的返回对象类型为bs4.element.Tag，为查找到的第一个满足条件的Tag；而find_all的返回对象为bs4.element.ResultSet（实际上就是Tag列表）,这里主要介绍find函数，find_all函数类似。\n",
    " >find(name=None, attrs={}, recursive=True, text=None, \\*\\*kwargs)\n",
    " \n",
    "注：其中name、attrs、text的值都支持正则匹配。\n",
    " >find_all(name=None, attrs={}, recursive=True, text=None, limit=None, \\*\\*kwargs)\n",
    "\n",
    "注：其中name、attrs、text的值都支持正则匹配。\n",
    "####(2) find函数的用法示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<p><a href=\"www.test.com\" class=\"mylink1 mylink2\">this is my link</a></p>'\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "a1 = soup.find('a')\n",
    "print type(a1)\n",
    "# 输出：<class 'bs4.element.Tag'>\n",
    "\n",
    "print a1.name\n",
    "print a1['href']\n",
    "print a1['class']\n",
    "print a1.text\n",
    "# 输出：\n",
    "'''\n",
    "a\n",
    "www.test.com\n",
    "[u'mylink1', u'mylink2']\n",
    "this is my link\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多个条件的正则匹配：\n",
    "import re\n",
    "a2 = soup.find(name = re.compile(r'\\w+'),class_ = re.compile(r'mylink\\d+'),text = re.compile(r'^this.+link$'))\n",
    "# 注：这里的class属性之所以写成'class_'，是为了防止和python关键字class混淆，其他属性名写正常的名就行，不用这样特殊处理\n",
    "print a2\n",
    "\n",
    "# 输出：\n",
    "'''\n",
    "<a class=\"mylink1 mylink2\" href=\"www.test.com\">this is my link</a>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find函数的链式调用\n",
    "a3 = soup.find('p').find('a')\n",
    "print a3\n",
    "\n",
    "# 输出：\n",
    "'''\n",
    "<a class=\"mylink1 mylink2\" href=\"www.test.com\">this is my link</a>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrs参数的用法\n",
    "# 注：支持正则匹配属性值（包括自定义属性）\n",
    "import re\n",
    "html = '<div class=\"myclass\" my-attr=\"123abc\"></div><div class=\"myclass\" my-attr=\"abc\">'\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "div = soup.find('div',attrs = {'class':'myclass','my-attr':re.compile(r'\\d+\\w+')})\n",
    "print div\n",
    "\n",
    "# 输出：\n",
    "'''\n",
    "<div class=\"myclass\" my-attr=\"123abc\"></div>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3 BeautifulSoup对象的类型\n",
    "Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象。所有对象可以归纳为4种类型: Tag , NavigableString , BeautifulSoup , Comment 。下面我们分别看看这四种类型都是什么东西。\n",
    "\n",
    "这个就跟HTML或者XML（还能解析XML？是的，能！）中的标签是一样一样的。我们使用find()方法返回的类型就是这个（插一句：使用find-all()返回的是多个该对象的集合，是可以用for循环遍历的。）。返回标签之后，还可以对提取标签中的信息。\n",
    "\n",
    "###3.1 TAG\n",
    "提取标签的名字：\n",
    "tag.name\n",
    "\n",
    "提取标签的属性：\n",
    "tag['attribute']\n",
    "我们用一个例子来了解这个类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html_doc, 'lxml')  #声明BeautifulSoup对象\n",
    "find = soup.find('p')  #使用find方法查到第一个p标签\n",
    "print(\"find's return type is \", type(find))  #输出返回值类型\n",
    "print(\"find's content is\", find)  #输出find获取的值\n",
    "print(\"find's Tag Name is \", find.name)  #输出标签的名字\n",
    "print(\"find's Attribute(class) is \", find['class'])  #输出标签的class属性值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.2 NavigableString\n",
    "NavigableString就是标签中的文本内容（不包含标签）。获取方式如下：\n",
    "tag.string\n",
    "还是以上面那个例子，加上下面这行，然后执行：\n",
    ">print('NavigableString is：', find.string)\n",
    "\n",
    "###3.3 BeautifulSoup\n",
    "BeautifulSoup对象表示一个文档的全部内容。支持遍历文档树和搜索文档树。\n",
    "\n",
    "###3.4 Comment\n",
    "这个对象其实就是HTML和XML中的注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup = \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\"\n",
    "soup = BeautifulSoup(markup)\n",
    "comment = soup.b.string\n",
    "type(comment)\n",
    "\n",
    "#有些时候，我们并不想获取HTML中的注释内容，所以用这个类型来判断是否是注释。\n",
    "\n",
    "if type(SomeString) == bs4.element.Comment:\n",
    "    print('该字符是注释')\n",
    "else:\n",
    "    print('该字符不是注释')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4 BeautifulSoup遍历方法\n",
    "###4.1 节点和标签名\n",
    "可以使用子节点、父节点、 及标签名的方式遍历："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.head #查找head标签\n",
    "soup.p #查找第一个p标签\n",
    "\n",
    "#对标签的直接子节点进行循环\n",
    "for child in title_tag.children:\n",
    "    print(child)\n",
    "\n",
    "soup.parent #父节点\n",
    "\n",
    "# 所有父节点\n",
    "for parent in link.parents:\n",
    "    if parent is None:\n",
    "        print(parent)\n",
    "    else:\n",
    "        print(parent.name)\n",
    "\n",
    "# 兄弟节点\n",
    "sibling_soup.b.next_sibling #后面的兄弟节点\n",
    "sibling_soup.c.previous_sibling #前面的兄弟节点\n",
    "\n",
    "#所有兄弟节点\n",
    "for sibling in soup.a.next_siblings:\n",
    "    print(repr(sibling))\n",
    "\n",
    "for sibling in soup.find(id=\"link3\").previous_siblings:\n",
    "    print(repr(sibling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.2 搜索文档树\n",
    "最常用的当然是find()和find_all()啦，当然还有其他的。比如find_parent() 和 find_parents()、 find_next_sibling() 和 find_next_siblings() 、find_all_next() 和 find_next()、find_all_previous() 和 find_previous() 等等。\n",
    "我们就看几个常用的，其余的如果用到就去看官方文档哦。\n",
    "\n",
    "find_all()\n",
    "搜索当前tag的所有tag子节点，并判断是否符合过滤器的条件。返回值类型是bs4.element.ResultSet。\n",
    "完整的语法：\n",
    "find_all( name , attrs , recursive , string , **kwargs )\n",
    "\n",
    "这里有几个例子\n",
    "\n",
    ">soup.find_all(\"title\")\n",
    "\n",
    "&#60;title>The Dormouse's story</title>\n",
    "\n",
    ">soup.find_all(\"p\", \"title\")\n",
    "\n",
    "&#60;p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "soup.find_all(\"a\")\n",
    "&#60;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "&#60;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
    "&#60;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
    "\n",
    ">soup.find_all(id=\"link2\")\n",
    "\n",
    "&#60;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
    "\n",
    ">import re\n",
    "soup.find(string=re.compile(\"sisters\"))\n",
    "\n",
    "u'Once upon a time there were three little sisters; and their names were\\n'\n",
    "\n",
    "name 参数：可以查找所有名字为 name 的tag。\n",
    "attr 参数：就是tag里的属性。\n",
    "string 参数：搜索文档中字符串的内容。\n",
    "recursive 参数： 调用tag的 find_all() 方法时，Beautiful Soup会检索当前tag的所有子孙节点。如果只想搜索tag的直接子节点，可以使用参数 recursive=False 。\n",
    "\n",
    "find()\n",
    "与find_all()类似，只不过只返回找到的第一个值。返回值类型是bs4.element.Tag。\n",
    "完整语法：\n",
    ">find( name , attrs , recursive , string , **kwargs )\n",
    "\n",
    "看例子：\n",
    "\n",
    ">soup.find('title')\n",
    "\n",
    "&#60;title>The Dormouse's story</title>\n",
    "\n",
    ">soup.find(\"head\").find(\"title\")\n",
    "\n",
    "&#60;title>The Dormouse's story</title>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
